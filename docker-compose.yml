# ─────────────────────────────────────────────────────────────────
# SLM Coding Council – Local Development Compose
# ─────────────────────────────────────────────────────────────────
# Usage:
#   docker compose up --build
#
# NOTE: The GPU-backed vLLM services require NVIDIA Container Toolkit.
#       For local CPU-only testing, comment out the agent services and
#       point the gateway .env at mock/remote endpoints instead.
# ─────────────────────────────────────────────────────────────────

services:
  # ── API Gateway ────────────────────────────────────────────────
  gateway:
    build: .
    ports:
      - "8080:8080"
    env_file: .env
    depends_on:
      - researcher
      - generator
      - debugger
      - tester
    restart: unless-stopped

  # ── Tech Researcher (Gemma 3 4B-IT) ───────────────────────────
  researcher:
    image: vllm/vllm-openai:latest
    ports:
      - "8001:8000"
    environment:
      - MODEL_ID=google/gemma-3-4b-it
      - MAX_MODEL_LEN=8192
      - GPU_MEMORY_UTILIZATION=0.90
    command: >
      --model google/gemma-3-4b-it
      --max-model-len 8192
      --gpu-memory-utilization 0.90
      --host 0.0.0.0
      --port 8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # ── Code Generator (Qwen3-Coder 4B) ───────────────────────────
  generator:
    image: vllm/vllm-openai:latest
    ports:
      - "8002:8000"
    environment:
      - MODEL_ID=Qwen/Qwen3-Coder-4B
      - MAX_MODEL_LEN=8192
      - GPU_MEMORY_UTILIZATION=0.90
    command: >
      --model Qwen/Qwen3-Coder-4B
      --max-model-len 8192
      --gpu-memory-utilization 0.90
      --host 0.0.0.0
      --port 8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # ── Debugger (DeepSeek-R1-Distill-Qwen-7B) ────────────────────
  debugger:
    image: vllm/vllm-openai:latest
    ports:
      - "8003:8000"
    environment:
      - MODEL_ID=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
      - MAX_MODEL_LEN=8192
      - GPU_MEMORY_UTILIZATION=0.90
    command: >
      --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
      --max-model-len 8192
      --gpu-memory-utilization 0.90
      --host 0.0.0.0
      --port 8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # ── Tester (Phi-4-mini) ───────────────────────────────────────
  tester:
    image: vllm/vllm-openai:latest
    ports:
      - "8004:8000"
    environment:
      - MODEL_ID=microsoft/phi-4-mini-instruct
      - MAX_MODEL_LEN=8192
      - GPU_MEMORY_UTILIZATION=0.90
    command: >
      --model microsoft/phi-4-mini-instruct
      --max-model-len 8192
      --gpu-memory-utilization 0.90
      --host 0.0.0.0
      --port 8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
